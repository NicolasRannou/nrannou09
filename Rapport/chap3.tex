\chapter{The contributions}\label{sec:contributions}
In this chapter we will present all the contribution to enhance the segmentation workflow in Slicer 3. We propose solution to the problem cited in the previous chapter (\ref{su:limitations}). We get started with the registration problem. The we propose solutions to enhance the class selection and to allow the user to evaluate his selection. Finally, we present some tools we added to help the user to find the good intensity normalization value and an estimation of the global prior weights.
%
\section{MRI Bias Field correction}

The registration step could present some problems if the image to segment has intensities inhomogeinities. We remind the problem, then we present the solution proposed.

\subsection{Interest}

In the segmentation process, a registration step is required. Registration consists in finding a trasformation to fit two images as well as possible. It is described in details in DSFSDF. Only one pre-processing (intensity normalization) is done before the registration. The problem is that the algorithm is designed to treat MR images. MR images are often corrupted by a bias field. Thus, the the image to register presents intensities inhomogeinities. These inhomogeinities can deteriore a lot the registration.
\par
On figure (\ref{fig:bfexemple}), we present the result of the registration between an atlas and a biased MR image. Note that the target MR image has been normalized to have the same mean value as the atlas. The results is clearly bad. A solution must be brought to enhance this step and so the segmentation.

  \begin{figure}[ht]\centering
  \includegraphics[width=0.6\textwidth]{Images/Screenshots/badRegistration.png}
  \caption{Result of registration of a biased MR image without correction}\label{fig:bfexemple}
  \end{figure}
  
\subsection{Our approach}

The idea simply consists in correcting the bias field of the MR image before this step. Thus, the registration will be significantly enhanced.Since the registration is better, it should also increase the segmentation.
\par
To correct the bias field, we used the non-parametric approach presented by Styner in SDFSDF. We choose a non-parametric approach because it doesn't require prior information like the number of tissue to correct or the mean value of each tissue to correct. We implement an ITK\footnote{open-source C++ toolkit for segmentation and registration. See \cite{13}.} filter (\cite{14}) in Slicer 3. 
\par
We choose not to implement it in Slicer 3 as part as the EM Segment module. Indeed, users may want to correct the bias field in MR images for other purposes. Moreover, because it would be the first pre-processing step, it is possible to do so. The user will first have to correct the intensities inhomogeinities via the module then use the corrected images in the EM segmentation module.
\par
We can describe the new segmentation workflow in Slicer 3 as we do in figure (\ref{fig:wfwbc}).



\begin{figure}[ht]\centering
  \includegraphics[width=1\textwidth]{Images/Graphics/bfnwf.png}
  \caption{New algorithm pipeline}\label{fig:wfwbc}
  \end{figure}
  
  
\par
After the bias correction, the result of the registration clearly appears to be better  (\ref{fig:goodRegistration}). 

\begin{figure}\centering
  \includegraphics[width=.6\textwidth]{Images/Screenshots/goodRegistration.png}
  \caption{Registration after bias correction.}\label{fig:goodRegistration}
\end{figure}
\par
We evaluated accuracy of the registration. We used the simple approach of the joint histogram.
The joint histogram evalutaion method is basic comparaison. Let $A$ be a matrix of size $W*L$. $W$ will be the intensity range of the first image used for the comparaison. $L$ will be the intensity range of the second image to be coompared. The matrix is initialized to $0$. Each time that in the same position, there is the same intensities in the two images, we add $1$ in the good cell in the matrix.
After the joint histogram creation, the value at the coordinate ${i,j}$ in the matrix is the number of pixel pairs having gray level $i$ at position ${x,y}$.
%\subsection{Results}
%corrected not corrected
%parameters explanation

%

\section{Class Distribution selection}\label{sec:CDS}
%
During parameters initialization, the user has to define each class distribution. The previous method of selection presented some limitations and we proposed a new approach.
%
\subsection{Interest}
%
So far, the user had two choices to define each class distribution. 
\par
The first possiblity consisted in entering manually the intensities mean value and variance for each class, for each volume to be processed. This way, the user can be very precise and accurate when he defines each class. But it is very hard to find the good mean value and variance for each class for each volume. Morever, each time we want to process a new volume, we have to redefine mean values and variances. It is not convenient and it can a lot of time to find accurate values for the parameter initialization. 
\par
The next approch consisted in defining a class model by manual sampling. For each class, the user clics in the related part of the volume. The problem with this method is that you compute your mean value and variance using only a few samples. Your sample will never be bigger that one hundred points because it is not convenient. Then, your mean values and variances are not accurate. Moreover, results are not reproducible with this method. This the number of samples is reduced, means and variances can vary a lot with one more sample and you can never reproduce two times the same initialization.
\par
Because of all these limitations,  we proposed a new approach using a label map, to estimate each class model.
%
\subsection{Method used}
%
The idea is to create a label map. This map contains colors. There is one color for each class we want to segment. The relation color/class is stored in $H$ (section ), in the EM algorithm. This relation color/class is set up during the tree creation step DSFSDF.
\par
The user creates a label map by coloring caracterisc regions for each tissue to segment, in the appropriate color. This gives a spatial information to the algorithm. It can now estimate automaticly the mean value and covariance of each class, for each tissue, using this label map.

\begin{figure}\centering
  \includegraphics[width=0.8\textwidth]{Images/Screenshots/labelmaps.png}
  \caption{Axial view of the label map.}\label{fig:labelmaps}
\end{figure}

\par 
It is very convenient because, since the algorithm needs a good initialization, we can easily define a sample of hundred of points for each class. The results will representatives. Moreover, the results are now reproducible. Indeed, we can store then re-use the same label map. The results will remain the same.
\\
NUMBERS TO PROVE DIFFERENCE!!!

%
\section{Class Distribution visualization}\label{sec:tables}

An important contribution is a tool which allows to visualize the distribution of the classes to be segmented.
\subsection{Interest}
As discussed before, the algorithm is sensible to the initalization. It means that the initialization has to be good. Once the parameters are chosen, the user has no tools to know if his selection is accurate. Two classes to segment can't have too close means and variances. Even if the user sees the values he chooses, it is not easy to know if two classes to be segmented are too similar or not.
\subsection{Our approach}
The objective is to provide the user the most accurate and usefull vizualisation as possible.
\par
We first assumed that each class has a normal distribution. We first decided to plot the gaussian in 3D, using the multivariate normal distribution.
In the 2-dimensional nonsingular case, the probability density function is 
\begin{equation*}
f(x,y)=\frac{1}{2 \pi \sigma_y \sigma_y \sqrt{1-\rho^2}} \operatorname*{exp}\Big ( -\frac{1}{2(1-\rho^2)}\Big( \frac{(x-\mu_x)^2}{\sigma_x^2} + \frac{(y-\mu_y)^2}{\sigma_y^2} - \frac{2\rho(x-\mu_x)(y-\mu_y)}{\sigma_x \sigma_y}\Big)\Big)   
\end{equation*}
(see \cite{15}). $x$ and $y$ are the position of the pixel in the 2D space. $f(x,y)$ will return the value (heigth) of the $(x,y)$ pixel. Each $X$ and  $Y$ axe represent one volume. Let's first say that the range of the $X$ and $Y$ axes are the intensity range of the $X$ and $Y$ volumes. $\mu_x$ is the mean value of the class in the $X$ volume. $\mu_y$ is the mean value of the class in the $Y$ volume. $\sigma_x$ and  $\sigma_y$ are the variance of the tissue in its respective volume. $\rho$ is the correlation between $X$ and $Y$. It indicates the strength and direction of a linear relationship between two random variables ( see \cite{16}). 
\par
We can easily deduce $\rho$ from the covariance matrix $\Sigma$ (see \cite{17}). Indeed, in the 2D case, the covariance matrix can be expressed as

\begin{equation*}
\mathbf{\Sigma} = 
 \begin{bmatrix}
   \sigma_x^2 & \rho \sigma_x \sigma_y \\
   \rho \sigma_x \sigma_y & \sigma_y^2
 \end{bmatrix}
\end{equation*}

The covariance matrix and the mean values for each class to segment for each image are computed during the labelmap sampling (section~\ref{sec:CDS}).
\par
We said that the range of the $X$ and $Y$ axes are the intensity range of the $X$ and $Y$ volumes. The problem with this approach appears if the classes to segment are not spread over all the intensities. Indeed, the vizualisation is not good then: the gaussian is only localized in a small portion of the 2D plane. We want to "zoom" on the region of interest. We decided to change the range of the two axes. Then range will be re-defined for each image. Let's present it for a given image $X$. It is now the difference between $Max$, the maximum value extracted with the label map, between all the samples between all the classes for the image $X$, and $Min$, its opposite.

IMAGES

\section{Intensity Normalization}
Another very usefull contribution is a tool which helps the user to determine the good normalization value.
\subsection{Interest}
As discussed in section (\ref{GUI}), at the step 4, an intensity normalization is done. Will already presented the utility of an intensity normalization in the same section. The problem is that the user has no tools to find the good values for the segmentation. He has to guess the mean intensity of the voxels in the MR image, background exluded. This is of course not doingable in practise.
\subsection{Our approach}
We implemented a simple tool, to allow the user to find easily and accurately this normalization value.
\par
The first step of the work consited in creating the histogram associated to the image. The $Y$ axe which presents the number of pixels for each intensity in the volume uses a log-scale because the range is huge. The log scale reduces considerably the range. We then added a cursor in this histogram. Using it, the user can choose the intensity which will be the "limit" between the background and part of interest. Finally, while the cursor is moving, our algorithm computes automatically the mean value of the voxels in the volume, from this intensity range to the highest intensity range. This is the normalization value.
\par
We present in figure (\ref{fig:intensitynormalization}) the tool we developped. A T1 volume has been loaded. The user can move the cursor in the histogram. While moving, it returns in real-time the normalization value for the given position of the cursor in the lower frame.

\begin{figure}\centering
  \includegraphics[width=0.5\textwidth]{Images/Screenshots/intensityNormalization.png}
  \caption{Tool developped for the intensity normalization parameter estimation}\label{fig:intensitynormalization}
\end{figure}
%


\section{Global Prior Estimation}

The last contribution to the EMS is a tool which provides the user an easy and fast way to estimate the global prior weights (GPW).(ref ch2 ...)

\subsection{Presentation of the problem}
This contribution is usefull in many different ways.
When you run the segmentation process,at the 6th step of the process, you have to provide to the algorithm an estimation of the GPW for each node in the tree. 
First of all if there are a lot of structures to segment, they user can spend a lot of time during this step. Indeed, for each part of this tree strcuture, they have to define the GPW. Moreover, the user may not know at all which weights to choose. This new approach will provide the users a good estimations of the weights to use. We must also keep in mind that the end users are physicists. They might don't understand what the parameters meanings and providing them a visual feedback could help them a lot.
\subsection{Our approach}
We divided the problem in two parts. The first part will be about providing the user a real-time feedback regarding the GPW estimation.
The second part will consist in developping an algorithm which fills automatically the tree.
\subsubsection{Fast user feedback}
We can divide the feedback part in 3 steps: the histogram computation and utilisation, the multicolumn list and the labelmap generated.
The histogram allows the user to manual segment classes based on intensity.
The multicolumn list allows the user to change the order of the classes in the histogram.
The labelmap provides to the user a visual feedback, base on the segmentation realized in the histogram.
Using these three complementary tools, the user, even if he is not initiated can estimate easily, accurately and rapidely the GWP.
\\schema
\subsubsection{Global priors evaluation}
The algorithm used to estimate the weight of each node is iterative. It starts from the root and goes to the leaves. It evaluates the weight of the childs of the active node at each iteration. Here is a description of the algorithms used to compute the GPW of each node.\\
DEscirption algo 1\\\\
%
%
\begin{minipage}{1\textwidth}
%
\hrule
\textbf{\\Algorithm 1:} \textsc{TreeWeightEstimation}(R, W)
\hrule
%variables definition
\textbf{\\define}  C = CHILD(R) $\leftarrow$ set of childrens of root R\\ 
\textbf{define}  LEAF(C)      $\leftarrow$ set of leaves of tree with roots C\\ 
\textbf{define}  H            $\leftarrow$ set of structure-specific information defined by LEAF(C) for each leaf\\
%
\textbf{update} W in childrens of root R with the results of \textsc{WeightEstimation}(C,LEAF(C),H)\\
%
\textbf{for each} node R' in CHILD(R) that is not a leaf\\
%
$\triangleright$\textsc{TreeWeightEstimation}(R', W)\\
\hrule
%
\end{minipage}
%
\\\\\\Descripton algo2\\
The algorithm used estimates the global prior of the leaves of the current node, based on the number of pixel which belong to the child classes.
This number of pixels is calculated from the segmentation computed in the histogram.(CF ..)\\\\
%
\begin{minipage}{1\textwidth}
%
\hrule
\textbf{\\Algorithm 2:} \textsc{WeightEstimation}(C,LEAF(C), W,H)
\hrule
%variables definition
\textbf{\\define}  T $\leftarrow$ set of total weight of leaves in LEAF(C). Leaves weights are contained in H\\ 
\textbf{define}  E      $\leftarrow$ set of weight for each node of C\\ 
\textbf{for each}  node of C\\
$\triangleright$E=E+H : Get the total weight of each node\\
%
W=E/T\\
%
\textbf{return} (W)\\
\hrule

\end{minipage}
\\\\\\
\par
The new tool is presented in figure (A) (\ref{fig:globalpriors}). When the user moves a cursor, it changes the intensity range for each class. The figure (B) (\ref{fig:globalpriors}) is updated in consequence and provides a real time feedback to the user, about about what he is doing. Clicking on "update tree" (in (A)), the estimated size of a class is computed and is automatically fills the information into the tree structure (parameter $H$).
\begin{figure}\centering
  \includegraphics[width=0.7\textwidth]{Images/Screenshots/GlobalPrior.png}
  \caption{Tool developped for the global prior weights estimation}\label{fig:globalpriors}
\end{figure}

%As we can see, the results obtained are good and accurate.The time of processing is fast and most of the people are now able to understand these parameters, even they are not familiar with EMS. This is an intuitive way to get an estimation of the GPW parameters.\\
%Of course some sfsdfs must be done.
%If there is a strong bias in the images we process, the pixels of a same class will have different values. Then, a segmentation based on intensity will provide bad results. Moreover, if the user wants to segment 2 classes which have the same color, just using the spatial information for example, they won't be able to use it properly.Thus, the user must always keep in mind that it is just an estimation and that he has to check if the values are accurate.
%

