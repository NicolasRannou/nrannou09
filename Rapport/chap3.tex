\chapter{Contributions}\label{sec:contributions}
In this chapter we will present all the contribution brought to enhance the segmentation workflow in Slicer 3. We propose solution to the problem cited in the previous chapter (\ref{su:limitations}). We propose solutions to enhance the class selection method and to allow the user to evaluate his selection. We deal with the registration problem in a third step. The Finally, we present some tools we added to help the user to find the good intensity normalization value and an estimation of the hierarchical parameters.
%
\section{Class Distribution selection}\label{sec:CDS}
%
During parameters initialization, the user has to define each class distribution. The previous method of selection presents some limitations and we propose a new approach.
%
\subsection{Interest}
%
So far, the user has two choices to define each class distribution. 
\par
The first possiblity consists in entering manually the intensities mean value and variance for each class, for each volume to be processed. This way, the user can be very precise and accurate when he defines each class. But it is very hard to find the good mean value and variance for each class for each volume. Morever, each time we want to process a new volume, we have to redefine mean values and variances. It is not convenient and it can a lot of time to find accurate values for the parameter initialization. 
\par
The next approch consisted in defining a class model by manual sampling. For each class, the user clics in the related part of the volume. The problem with this method is that you compute your mean value and variance using only a few samples. Your sample will never be bigger that one hundred points because it is not convenient. Then, your mean values and variances are not accurate. Moreover, results are not reproducible with this method. This the number of samples is reduced, means and variances can vary a lot with one more sample and you can never reproduce two times the same initialization.
\par
Because of all these limitations,  we proposed a new approach using a label map, to estimate each class model.
%
\subsection{Method used}
%
The idea is to create a label map. This map contains colors. There is one color for each class we want to segment. The relation color/class is stored in parameter $H$, in the EM algorithm. This relation color/class is set up during the tree creation step (section  \ref{GUI}).
\par
The user creates a label map by coloring caracterisc regions for each tissue to segment, in the appropriate color. This gives a spatial information to the algorithm. It can now estimate automaticly the mean value and covariance of each class, for each tissue, using this label map. The reason why covariance matrix is estimated, instead of the simple variance is presented in the next section.
\par 
It is very convenient because, since the algorithm needs a good initialization, we can easily define a sample of hundred of points for each class. The results will representatives. Moreover, the results are now reproducible. Indeed, we can store then re-use the same label map. The results will remain the same.

Figure (\ref{fig:labelmaps}) represent an example of a label map. (A) represent an axial view of the vloume to be segmented and (B) the label map created for this volume. Each color represents a tissue to be segmented.

\begin{figure}\centering
  \includegraphics[width=0.9\textwidth]{Images/Screenshots/labelmaps.png}
  \caption{Axial view of the label map.}\label{fig:labelmaps}
\end{figure}
\subsection{Evaluation}
To estimate the influence of the contribution, we processed to a simple comparaison. We selected manually ten points of the white matter as fine as possible. With this sample, we estimate mean value and covariance matrix, through 2 volumes. The second step of the evaluation consisted in estimating the same values for the same tissue using a label map (sample bigger than 200 points). \\ \\
\textbf{Results:}

\begin{equation*}
\mu_{Manual1} =  543 \mbox{~~,~~} \mu_{Manual2} =  93 \mbox{~~and~~} \mathbf{\Sigma_{Manual}} = 
 \begin{bmatrix}
   1105 & -25 \\
   -25 & 1308
 \end{bmatrix}
\end{equation*}

\begin{equation*}
\mu_{Label1} =  489 \mbox{~~,~~} \mu_{Label2} =  92 \mbox{~~and~~} \mathbf{\Sigma_{Label}} = 
 \begin{bmatrix}
   592 & -201 \\
   -201 & 280
 \end{bmatrix}
\end{equation*}

\par
The mean values ($\mu_{Manual}$ and $\mu_{Labell}$) differs slightly and covariance matrices ($\Sigma_{Manual}$ and $\Sigma_{Label}$) differ significantly. It means that our approach is usefull and it shows the importance of having a large sample to evaluate means and covariance values in a such process. The square variance of the class, is in position $(1,1)$ in the covariance matrix for the first volume. For the second volume, it is in position $(2,2)$. Variance expresses the range through which the class is expected to be. The estimation of this interval is way more precise with label map than with manual sampling. We can explain it with the important number of samples used for the estimation.

%
\section{Class Distribution visualization}\label{sec:tables}

An important contribution is a tool which allows to visualize the distribution of the classes to be segmented.
\subsection{Interest}
As discussed before, the algorithm is sensible to the initalization. It means that the initialization has to be good. Once the parameters are chosen, the user has no tools to know if his selection is accurate. Two classes to segment can't have too close means and variances. Even if the user sees the values he chooses, it is not easy to know if two classes to be segmented are too similar or not.
\subsection{Our approach}
The objective is to provide the user the most accurate and usefull vizualisation as possible.
\par
We first assumed that each class has a normal distribution. We first decided to plot the gaussian in 3D, using the multivariate normal distribution.
In the 2-dimensional nonsingular case, the probability density function is 

\begin{equation}\label{GDDDPGDGS}
f(x,y)=\frac{1}{2 \pi \sigma_y \sigma_y \sqrt{1-\rho^2}} \operatorname*{exp}\Big ( -\frac{1}{2(1-\rho^2)}\Big( \frac{(x-\mu_x)^2}{\sigma_x^2} + \frac{(y-\mu_y)^2}{\sigma_y^2} - \frac{2\rho(x-\mu_x)(y-\mu_y)}{\sigma_x \sigma_y}\Big)\Big)   
\end{equation}

(see \cite{15}). $x$ and $y$ are the position of the pixel in the 2D space. $f(x,y)$ will return the value (heigth) of the $(x,y)$ pixel. Each $X$ and  $Y$ axe represent one volume. Let's first say that the range of the $X$ and $Y$ axes are the intensity range of the $X$ and $Y$ volumes. $\mu_x$ is the mean value of the class in the $X$ volume. $\mu_y$ is the mean value of the class in the $Y$ volume. $\sigma_x$ and  $\sigma_y$ are the variance of the tissue in its respective volume. $\rho$ is the correlation between $X$ and $Y$. It indicates the strength and direction of a linear relationship between two random variables ( see \cite{16}). 


\par
We can easily deduce $\rho$ from the covariance matrix $\Sigma$ (see \cite{17}). Indeed, in the 2 dimensional case, the covariance matrix can be expressed as

\begin{equation*}
\mathbf{\Sigma} = 
 \begin{bmatrix}
   \sigma_x^2 & \rho \sigma_x \sigma_y \\
   \rho \sigma_x \sigma_y & \sigma_y^2
 \end{bmatrix}
\end{equation*}

The covariance matrix and the mean values for each class to segment for each image are computed during the labelmap sampling (section~\ref{sec:CDS}). 
\par
Please note that if $\rho = 1$, it means that the two radom variables are the same and we can't use the same density probability function anymore. We use the classic normal distribution's formulation
\begin{equation}\label{GDPGDGS}
f(x,y)=A\operatorname*{exp}-\Big ( \frac{x-\mu_x}{2\sigma_X^2} + \frac{y-\mu_y}{2\sigma_Y^2}\Big)   
\end{equation}

$A$ is the amplitude of the gaussian.
\par
We said that the range of the $X$ and $Y$ axes are the intensity range of the $X$ and $Y$ volumes. The problem with this approach appears if the classes to segment are not spread over all the intensities. Indeed, the vizualisation is not good then: the gaussian is only localized in a small portion of the 2D plane. We want to "zoom" on the region of interest. We decided to change the range of the two axes. Then range will be re-defined for each image. Let's present it for a given image $X$. It is now the difference between $Max$, the maximum value extracted with the label map, between all the samples between all the classes for the image $X$, and $Min$, its opposite.
\par
For our particular purpose of tissues' distributions' vizualisation we didn't use exactly the formulas (\ref{GDDDPGDGS}) and (\ref{GDPGDGS}). Indeed, we don't normalize the curves: we set $\frac{1}{2 \pi \sigma_y \sigma_y \sqrt{1-\rho^2}}$ to $1$ in (\ref{GDDDPGDGS}) and $A$ to $1$ in (\ref{GDPGDGS}). We do it because we have no clue about the importance of each class so we don't want to "disavantage" any one. A compromise could be to have an amplitude factor proportional to the number of pixels which are supposed to constitute the class.
\par 
We finally obtain the results figure (\ref{fig:intensitynormalization}) for different sampling methods.
\begin{figure}\centering\label{classdistribution}
  \includegraphics[width=0.9\textwidth]{Images/Screenshots/classdistribution.png}
  \caption{Distribution of the class to be segmented}\label{fig:intensitynormalization}
 \end{figure}
 
(A) present the visualization we obtain after a manual sampling and (B) the results after a label map sampling. The distributions slightly differ. The dark blue point, on the left corner represents the air. The skull is represented in purple, the white matter in yellow, the gray matter in green and the cerebrospinal fluid (CSF) in red. The X axis represents the T1 volume and the Y axis the T2 volume. On (A), the skull is significantly more spreaded than in (B), especially along the X axe (T1). According to an expert, this result is better in the case of labelmap sampling, indeed, the skull should almost have the same variance through T1 and T2, what is almost the case in (B). Moreover, regarding the CSF and the grey matter, the distribution is better in (B) again. According to the same expert, the intensities of CSF and grey matter, in the T1 volume are close and can't be as distant as in (A). These two observations show the utility of the labelmap sampling in order to have accurate tissue distributions.


\section{MRI Bias Field correction}\label{biasfieldcorrectionregistration}

The registration step could present some problems if the image to segment has intensities inhomogeinities. Moreover, bias field canal present problems regarding the classes distribution. We will first remind the problems, then we present the solutions proposed.

\subsection{Interest}

In the segmentation process, a registration step is required. Registration consists in finding a trasformation to fit two images as well as possible. The main methods are described in \cite{21}. Only one pre-processing (intensity normalization) is done before the registration. The problem is that the algorithm is designed to treat MR images. MR images are often corrupted by a bias field. Thus, the image to register present intensities inhomogeinities. These inhomogeinities can deteriore a lot the registration.
\par
On figure (\ref{fig:bfexemple}), we present the result of the registration between an atlas and a biased MR image. Note that the target MR image has been normalized to have the same mean value as the atlas. The results is clearly bad. A solution must be brought to enhance this step and so the segmentation.

  \begin{figure}[ht]\centering
  \includegraphics[width=0.5\textwidth]{Images/Screenshots/badRegistration.png}
  \caption{Result of registration of a biased MR image without correction}\label{fig:bfexemple}
  \end{figure}
  
\par
Moreover, this bias field lead to another main issue. Indeed, if the MR image is biased, a the intensity of a given tissue will vary a lot through the volume, even if it should not. Then the algorithm will be initialized with wrong mean and variance values and the segmentation won't be as good as it would be with a corrected image.
  
\subsection{Our approach}

The idea simply consists in correcting the bias field of the MR image before this step. Thus, the registration will be significantly enhanced.Since the registration is better, it should also increase the segmentation.
\par
To correct the bias field, we used the non-parametric approach presented by Sled \textit{et al} in \cite{19}. We choose a non-parametric approach because it doesn't require prior information like the number of tissues to correct or the mean value of each tissue to be corrected. We implement an ITK\footnote{open-source C++ toolkit for segmentation and registration. See \cite{13}.} filter (\cite{14}) in Slicer 3. 
\par
We can describe the new segmentation workflow in Slicer 3 as we do in figure (\ref{fig:wfwbc}).

\begin{figure}[ht]\centering
  \includegraphics[width=1\textwidth]{Images/Graphics/newalgo.png}
  \caption{New algorithm pipeline}\label{fig:wfwbc}
  \end{figure}

\par
We choose not to implement it in Slicer 3 as part as the EM Segment module. Indeed, users may want to correct the bias field in MR images for other purposes. Moreover, because it would be the first pre-processing step, it is possible to do so. The user will first have to correct the intensities inhomogeinities via the module then use the corrected images in the EM segmentation module.

  
  
\par
After the bias correction, we obtain interesting results (\ref{fig:goodRegistration}). (A) represents the atlas to be registered. (B) represents the target volume for the registration. (C) repre sents the atlas after the registration. The result of the registration visually appears to be better but we can't be satisfied of this visual verification. We need a formal evaluation method.

\begin{figure}\centering
  \includegraphics[width=.5\textwidth]{Images/Screenshots/goodRegistration.png}
  \caption{Registration after bias correction.}\label{fig:goodRegistration}
\end{figure}

\subsection{Evaluation}

We evaluated accuracy of the registration using the joint histograms method.
The joint histogram evaluation method is basic comparaison between two images. Let $A$ be a matrix of size $W*L$. $W$ will be the intensity range of the first image used for the comparaison. $L$ will be the intensity range of the second image to be compared. The matrix is initialized to $0$. Each time that in the same position, there is the same intensities in the two images, we add $1$ in the corresponding cell in the matrix. Thus, a perfect registration, would lead to an array of zeros, expect on the diagonal.
After the joint histogram creation, the value at the coordinate ${i,j}$ in the matrix is the number of pixel pairs having gray level $i$ at position ${x,y}$.
\par
In figure (\ref{fig:joint1}), (A) and (B) compares the joint-histogram of the biased image and its atlas, respectively before and after registration.  The color scale used is the following one: if there are a lot of pixels in a cell of the array, the cell will be displayed in red. We the number of points decreases continously. Red becomes orange, yellow, green then blue.

\begin{figure}\centering
  \includegraphics[width=.9\textwidth]{Images/Screenshots/jointhistograms1.png}
  \caption{Joint histograms to evaluate the registration.}\label{fig:joint1}
\end{figure}


In figure (\ref{fig:joint2}), (A) and (B) compares the joint-histograms of the corrected image and its atlas, before (A) and after (B) registration.

\begin{figure}\centering
  \includegraphics[width=.9\textwidth]{Images/Screenshots/jointhistograms2.png}
  \caption{Joint histograms to evaluate the registration.}\label{fig:joint2}
\end{figure}

It clearly appears the the difference between before and after registration is more significant if the bias field is corrected. Indeed, we compare the biased joint histogram, there is no significative difference between before and after registration. That means that the registration didn't improve the similarity between the images. On the contrary, if we correct the bias field, it the number of point around the matix's diagonal clearly increases. It means that now, the images are more similiars than before. It shows the utility of the contribution. The influence of these results after the whole segmentation process will be presented in the next chapter.
\par
Regarding the tissue intensity distribution effect, we will present some results using the tool we developped in the previous section, to evaluate the classes' distribution.
Using the same labelmap for sampling, we obtain two totaly different distributions. Figure (\ref{fig:biasedunbiased}) clearly show the two different distributions. The relation tissue/color is the same as the one in figure (\ref{fig:intensitynormalization}). (A) presents the distribution before bias correction. (B) presents the distribution through the corrected volume.

\begin{figure}\centering
  \includegraphics[width=.9\textwidth]{Images/Screenshots/biasedunbiased.png}
  \caption{Tissue distributions.}\label{fig:biasedunbiased}
\end{figure}
The tissues variances are huge, especially for the CSF (red) through T2. Its distribution through the T2 volume is huge. According to an expert, it should clearly not. It should only be in the high intensities of the T2 volume. Our result means that the CSF class contents high and low intensities. It is due to the bias and is not acceptable.
\par
Intuitively, we understand that the segmentation process should be a lot deteriored because of the two issues we presented (registration and distribution).
%\subsection{Results}
%corrected not corrected
%parameters explanation
\subsection{Registration parameters}
Even if we are doing a non-parametric registration, some parameters has to be defined. "Non-parametric" means no information about the volume and classes to correct. We will first present and explain you the parameters. In a second step, we will propose you some parameters adaptated to different problems.

\begin{itemize}
\item \textbf{Shrink factor}\\
\hspace*{4 mm} It is a factor which is used to reduce the size of the image to be processed. A down-sampling is done by the bias correction filter.

\item \textbf{Maximum Number Of Iterations}\\
\hspace*{4 mm} Optimization of the bias field occurs iterativly until the number of iterations exceeds the maximum specified by this variable.

\item \textbf{Bias Field Full Width At Maximum Iteration}\\
\hspace*{4 mm} The bias field is modelled with a Gaussian. This variable characterizes this Gaussian (see \cite{20}) and can be presented as a parameter which defines the strength of the bias.
\end{itemize}

\par
From the understanding of the parameters, it is now obvious that if you want to increase the time of processing, you should increase the shrink factor or reduce the maximum number of iteration. The limitation is that is can deteriore the bias field correction. You can also increase or reduce the Bias field full width at maximum iteration, depending on the importance of the bias.
%


\section{Intensity Normalization}\label{intensitynormalizationddd}
Another very usefull contribution is a tool which helps the user to determine the good normalization value.
\subsection{Interest}
As discussed in section (\ref{GUI}), at the step 4, an intensity normalization is done. Will already presented the utility of an intensity normalization in the same section. The problem is that the user has no tools to find the good values for the segmentation. He has to guess the mean intensity of the voxels in the MR image, background exluded. This is of course not doingable in practise.
\subsection{Our approach}
We implemented a simple tool, to allow the user to find easily and accurately this normalization value.
\par
The first step of the work consited in creating the histogram associated to the image. The $Y$ axe which presents the number of pixels for each intensity in the volume uses a log-scale because the range is huge. The log scale reduces considerably the range. We then added a cursor in this histogram. Using it, the user can choose the intensity which will be the "limit" between the background and part of interest. Finally, while the cursor is moving, our algorithm computes automatically the mean value of the voxels in the volume, from this intensity range to the highest intensity range. This is the normalization value.
\par
We present in figure (\ref{fig:intensitynormalization2}) the tool we developped. A T1 volume has been loaded. The user can move the cursor in the histogram. While moving, it returns in real-time the normalization value for the given position of the cursor in the lower frame.

\begin{figure}\centering
  \includegraphics[width=0.5\textwidth]{Images/Screenshots/intensityNormalization.png}
  \caption{Tool developped for the intensity normalization parameter estimation}\label{fig:intensitynormalization2}
\end{figure}
%


\section{Global Prior Weights Estimation}\label{GPSPDGSPGS}

The last contribution to the EMS is a tool which provides the user an easy and fast way to estimate the approximate size (also called Global Prior Weights (GPW)) of each class to be segmented.

\subsection{Presentation of the problem}
During the segmentation process,at the $6^{th}$ step of the intialization (section \ref{GUI}), you have to provide to the algorithm an estimation of the size of each tissue to be segmented. 
First of all if there are a lot of structures to segment, they user can spend a lot of time during this step. Moreover, the user may not know at all which size to choose. A tool to estimate of the good sizes to choose is needed. 
%We must also keep in mind that the end users are physicists. They might don't understand what the parameters meanings and providing them a visual feedback could help them a lot.
\subsection{Our approach}
We divided the problem in two parts. The first part will be about providing the user a real-time feedback regarding the global prior weights estimation.
The second part will consist in developping an algorithm which fills automatically the tree.
%\subsubsection{Fast user feedback}
%We can divide the feedback part in 3 steps: the histogram computation and utilisation, the multicolumn list and the labelmap generated.
%The histogram allows the user to manual segment classes based on intensity.
%The multicolumn list allows the user to change the order of the classes in the histogram.
%The labelmap provides to the user a visual feedback, base on the segmentation realized in the histogram.
%Using these three complementary tools, the user, even if he is not initiated can estimate easily, accurately and rapidely the GWP.
%\\schema
%\subsubsection{Global prior weights evaluation}
%The algorithm used to estimate the weight of each node is iterative. It starts from the root and goes to the leaves. It evaluates the weight of the childs of the active node at each iteration. Here is a description of the algorithms used to compute the GPW of each node.\\
%DEscirption algo 1\\\\
%
%
%\begin{minipage}{1\textwidth}
%
%\hrule
%\textbf{\\Algorithm 1:} \textsc{TreeWeightEstimation}(R, W)
%\hrule
%variables definition
%\textbf{\\define}  C = CHILD(R) $\leftarrow$ set of childrens of root R\\ 
%\textbf{define}  LEAF(C)      $\leftarrow$ set of leaves of tree with roots C\\ 
%\textbf{define}  H            $\leftarrow$ set of structure-specific information defined by LEAF(C) for each leaf\\
%
%\textbf{update} W in childrens of root R with the results of \textsc{WeightEstimation}(C,LEAF(C),H)\\
%
%\textbf{for each} node R' in CHILD(R) that is not a leaf\\
%
%$\triangleright$\textsc{TreeWeightEstimation}(R', W)\\
%\hrule
%
%\end{minipage}
%
%\\\\\\Descripton algo2\\
%The algorithm used estimates the global prior of the leaves of the current node, based on the number of pixel which belong to the child classes.
%This number of pixels is calculated from the segmentation computed in the histogram.(CF ..)\\\\
%%
%\begin{minipage}{1\textwidth}
%
%\hrule
%\textbf{\\Algorithm 2:} \textsc{WeightEstimation}(C,LEAF(C), W,H)%
%\hrule
%variables definition
%\textbf{\\define}  T $\leftarrow$ set of total weight of leaves in LEAF(C). Leaves weights are contained in H\\ 
%\textbf{define}  E      $\leftarrow$ set of weight for each node of C\\ 
%\textbf{for each}  node of C\\
%$\triangleright$E=E+H : Get the total weight of each node\\
%
%W=E/T\\
%
%\textbf{return} (W)\\
%\hrule

%\end{minipage}
%\\\\\\

  \begin{figure}\centering
  \includegraphics[width=0.7\textwidth]{Images/Screenshots/GlobalPrior.png}
  \caption{Tool developped for the global prior weights estimation}\label{fig:globalpriors}
  \end{figure}

The new tool is presented in figure (\ref{fig:globalpriors}) (A). When the user moves a cursor, it changes the intensity range for each class. The tool provides a real time feedback to the user, about about what he is doing. The user sees figure \ref{fig:globalpriors}) (B) which is updated in real time, regarding the position of all the cursors consequence. Clicking on "update tree" (in figure \ref{fig:globalpriors}) (A)), the estimated size of a class is computed and the information is stored into the tree structure (parameter $H$).\\
\par
We presented the contribution we did to the EM semgentation module in Slicer 3. It lookds usefull, nevertheless we still don't know if it will have a real impact on the whole segmentation process. In the next chapter, we present the influence of the contributions on the whole segmentation proocess.

%As we can see, the results obtained are good and accurate.The time of processing is fast and most of the people are now able to understand these parameters, even they are not familiar with EMS. This is an intuitive way to get an estimation of the GPW parameters.\\
%Of course some sfsdfs must be done.
%If there is a strong bias in the images we process, the pixels of a same class will have different values. Then, a segmentation based on intensity will provide bad results. Moreover, if the user wants to segment 2 classes which have the same color, just using the spatial information for example, they won't be able to use it properly.Thus, the user must always keep in mind that it is just an estimation and that he has to check if the values are accurate.
%

