\chapter{Contributions}\label{sec:contributions}
In this chapter we will present all the contribution realized to enhance the segmentation workflow in Slicer 3. 
%We propose some solutions to the problems mentionned in the previous chapter (\ref{su:limitations}). 
We propose solutions to enhance the class selection method and to allow the user to evaluate the selection. We will address the registration problem in a third step. Finally, we present the tools that we have added to help the user to find the optimum intensity normalization value and to estimate the hierarchical parameters.
%
\section{Class Distribution Selection}\label{sec:CDS}
%
During parameters initialization, the user has to define each class distribution. The previous method of selection presents some limitations and we propose a new approach.
%
\subsection{Motivation}
%
So far, the user has two choices to define each class distribution. 
\par
The first possiblity consists of entering manually the intensities mean value and variance for each class, for each volume to be processed. This way, the user is precise to define each class. However it is challenging to find the optimum mean value and variance for each class for each volume. Morever, each time the user wants to process a new volumen, mean values and variances have to be redefined. It is not convenient and it can be time-consuming to find accurate values for the parameter initialization. 
\par
The next approach consists of defining a class model by manual sampling. For each class, the user clicks in the related part of the volume. The problem with this method is that the computation of the mean value and variance only uses a few samples. Because manual sampling is time consuming, the user will only select a reduced number of sample compared to the total number of voxels in the volume. Consequently mean values and variances are not accurate. Besides, reproducibility is an issue. Indeed it is challenging for an user to perform an identical sampling on two volumes.
\par
Because of all these limitations,  we proposed a new approach using a labelmap, to estimate each class model.
%
\subsection{Solution}
%
Our solution is to create a labelmap where each tissue class is given a label and a color. The relation color/class is stored in parameter $H$, in the EM algorithm. This relation color/class is set up during the tree creation step (section  \ref{GUI}).
\par
The user creates a labelmap by coloring characterisc regions for each tissue to segment in the appropriate color. This gives a spatial information to the algorithm. The labelmap allows to compute automatically the mean value and covariance of each class, for each tissue. In the next section we explain why we compute the covariance matrix instead of the variance.
\par 
The labelmap sampling is a convenient solution because a large number of sample can be drawn for each class. Besides the sampling is now reproducible since we can store then re-use it later.

Figure (\ref{fig:labelmaps}) illustrates an example of a labelmap. Left image (A) represents an axial view of the T1 volume to be segmented and right image (B) the labelmap created for this volume. Each color represents a tissue to be segmented.

\begin{figure}\centering
  \includegraphics[width=0.9\textwidth]{Images/Screenshots/labelmaps.png}
  \caption{Axial view of the labelmap}{Left image (A) represents an axial view of the T1 volume to be segmented. Right image (B) represents the volume overlays by the labelmap. Pink represents the air. White represents the skull. Yellow represents the white matter (WM). Blue represents the grey matter (GM). Red represents the cerebrospnial fluid(CSF).}\label{fig:labelmaps}
\end{figure}
\subsection{Evaluation}
To estimate the importance of this contribution, we perform a simple comparison. We select manually ten points of the white matter as accurately as possible. With this manual sampling method, we estimate mean value and covariance matrix, within two volumes. The second step of the evaluation consists of estimating the same values for the same tissue using a labelmap (more than 200 sampling points). \\ \\
\textbf{Results:}

\begin{equation*}
\mu_{Manual1} =  543 \mbox{~~,~~} \mu_{Manual2} =  93 \mbox{~~and~~} \mathbf{\Sigma_{Manual}} = 
 \begin{bmatrix}
   1105 & -25 \\
   -25 & 1308
 \end{bmatrix}
\end{equation*}

\begin{equation*}
\mu_{Label1} =  489 \mbox{~~,~~} \mu_{Label2} =  92 \mbox{~~and~~} \mathbf{\Sigma_{Label}} = 
 \begin{bmatrix}
   592 & -201 \\
   -201 & 280
 \end{bmatrix}
\end{equation*}

\par
The mean values ($\mu_{Manual}$ and $\mu_{Label}$) differ slightly and covariance matrices ($\Sigma_{Manual}$ and $\Sigma_{Label}$) differ significantly. It means that our approach is useful and it shows the importance of having a large sample to evaluate means and covariance values. The square variance of the class  for the first volume, is in position $(1,1)$ in the covariance matrix. For the second volume, it is in position $(2,2)$. Variance expresses the range through which the class is expected to be. The estimation of this interval is significantly more accurate with labelmap sampling than manual sampling. We can explain it with the important number of samples used for the estimation.

%
\section{Class Distribution visualization}\label{sec:tables}

An important contribution is a tool which allows to visualize the distribution of the classes to be segmented.

\subsection{Motivation}
As discussed before, the algorithm is sensible to the initalization. In other words, the quality of the initialization is important. Once the parameters are chosen, the user has no tools to know if his selection is accurate. Two classes to segment can not have too close means and variances and even if the user sees the values obtained after sampling, it is challenging to know if two classes to be segmented are too similar or not.
\subsection{Solution}
The objective is to provide the user the most accurate and usefull vizualisation tool as possible.
\par
We first assume that each class has a normal distribution. This assumption is correct since the EM algorithm present in Slicer 3 is based on Gaussian mixture models. We decided to plot the Gaussian in the 3D space, using the multivariate normal distribution.
In the 2D non-singular case, the probability density function is 

\begin{equation}\label{GDDDPGDGS}
f(x,y)=\frac{1}{2 \pi \sigma_y \sigma_y \sqrt{1-\rho^2}} \operatorname*{exp}\Big ( -\frac{1}{2(1-\rho^2)}\Big( \frac{(x-\mu_x)^2}{\sigma_x^2} + \frac{(y-\mu_y)^2}{\sigma_y^2} - \frac{2\rho(x-\mu_x)(y-\mu_y)}{\sigma_x \sigma_y}\Big)\Big)   
\end{equation}

(see \cite{15}). $x$ and $y$ are the positions of the pixel in the plane. $f(x,y)$ will return the value (height) of the $(x,y)$ pixel. The $X$ axis represents a volume while the $Y$ axis represents another volume. Let's first say that the range of the $X$ and $Y$ axes are the intensity range of the $X$ and $Y$ volumes. $\mu_x$ is the mean value of the class in the $X$ volume. $\mu_y$ is the mean value of the class in the $Y$ volume. $\sigma_x$ and  $\sigma_y$ are the variance of the tissue in its respective volume. $\rho$ is the correlation between $X$ and $Y$. It indicates the strength and direction of a linear relationship between two random variables ( see \cite{16}). 


\par
We can easily deduce $\rho$ from the covariance matrix $\Sigma$ (see \cite{17}). Indeed, in the 2 dimensional case, the covariance matrix can be expressed as

\begin{equation*}
\mathbf{\Sigma} = 
 \begin{bmatrix}
   \sigma_x^2 & \rho \sigma_x \sigma_y \\
   \rho \sigma_x \sigma_y & \sigma_y^2
 \end{bmatrix}
\end{equation*}

The covariance matrix and the mean values for each class to segment for each image are computed during the labelmap sampling (section~\ref{sec:CDS}). 
\par
Please note that if $\rho = 1$, it means that the two radom variables are the same and we can not use the same density probability function anymore. We express the classic normal distribution
\begin{equation}\label{GDPGDGS}
f(x,y)=A\operatorname*{exp}-\Big ( \frac{x-\mu_x}{2\sigma_X^2} + \frac{y-\mu_y}{2\sigma_Y^2}\Big)   
\end{equation}

$A$ is the amplitude of the Gaussian.
\par
We said that the range of the $X$ and $Y$ axes are the intensity range of the $X$ and $Y$ volumes. The problem with this approach appears if the classes to segment are not spread over all the intensities. Indeed, the vizualisation is poor then: the Gaussian is only localized in a small portion of the 2D plane. We want to "zoom" on the region of interest. We decided to change the range of the two axes. Then range will be redefined for each image. Let's present it for a given image $X$. The range is now defined as the difference between $Max$, the maximum value extracted with the labelmap, between all the samples between all the classes for the image $X$, and $Min$, its opposite.
\par
For our particular purpose of tissues distributions vizualisation we did not use exactly the formulas (\ref{GDDDPGDGS}) and (\ref{GDPGDGS}). Indeed, we do not normalize the curves: we set $\frac{1}{2 \pi \sigma_y \sigma_y \sqrt{1-\rho^2}}$ to $1$ in Equations \ref{GDDDPGDGS} and $A$ to $1$ in Equation \ref{GDPGDGS}. We do it because we do not know the importance of each class so we do not want to "disavantage" any one in the visualization. A compromise could be to have an amplitude factor proportional to the number of pixels which constitute the class.
\subsection{Evaluation}
\par 
We finally obtain the results Figure \ref{fig:intensitynormalization} for different sampling methods.
\begin{figure}\centering\label{classdistribution}
  \includegraphics[width=0.9\textwidth]{Images/Screenshots/classdistribution.png}
  \caption{Distribution of the class to be segmented in an homogeneous volume}{Left image (A) represents the distribution of the class after a manual sampling. Right (B) image represents the class distribution after a labelmap sampling. Air is plotted in dark blue, skull in blue, WM in yellow, GM in green and CSF in red. X axis represents a T1 volume and Y axis represents a T2 volume.}\label{fig:intensitynormalization}
 \end{figure}
The left Figure (A) illustrates the visualization we obtain after a manual sampling and (B) the results after a labelmap sampling. The distributions slightly differ. The dark blue point, on the left corner represents the air. The skull is represented in purple, the white matter in yellow, the gray matter in green and the cerebrospinal fluid (CSF) in red. The X axis represents the T1 volume and the Y axis the T2 volume. In left image (A), the skull is significantly more spread than in right image (B), especially along the X axis (T1). According to a clinician expert, the labelmap sampling offers better results. Indeed, the skull should almost have the same variance in the T1 and T2 images, which is illustrated in Figure (B). According to the same expert, the intensities of CSF and grey matter, in the T1 volume can not be as distant as these are in image (A). The distributions of these two tissues should be close, as it is in right image (B). These two observations validates the utility of the labelmap sampling in order to have accurate tissue distributions.


\section{MRI Bias Field Correction}\label{biasfieldcorrectionregistration}

The registration step could present some problems if the image to segment exhibits intensity inhomogineity. Moreover, bias field can also be a problem regarding the accuracy of the class distributions. We will first remind the problems and then propose some solutions.

\subsection{Motivation}

In the segmentation process, a registration step is required. Registration consists of finding a transformation to fit two images as well as possible. The main methods are described in \cite{21}. Only one pre-processing (intensity normalization) is done before the registration. The problem is that the algorithm is designed to process MR images. MR images are often corrupted by a bias field. Thus, the image to register exhibits intensity inhomogeneity. These inhomogeinities significantly affect the registration.
\par
On Figure \ref{fig:bfexemple}, we present the result of the registration between an atlas (left image (A)) and a biased MR image (right image (B)). Note that the target MR image has been normalized to have the same mean value as the atlas. The image (C) shows that the registration is poor.

  \begin{figure}[ht]\centering
  \includegraphics[width=0.5\textwidth]{Images/Screenshots/badRegistration.png}
  \caption{Registration of a MR image which exhibits intensity inhomogeneity}{Image (A) is the atlas to be registered. Image (B) is the target of the registration. Image (C) is the image (A) after registration to image (B)}\label{fig:bfexemple}
  \end{figure}
  
\par
Moreover this bias field leads to another important issue. Indeed, if the MR image intensities are biased, the intensity of a given tissue varies significantly  in the volume, even if it should not. Then the algorithm will be initialized with wrong mean and variance values and the segmentation will not be as good as it should be.
  
\subsection{Solution}

The idea simply consists of correcting the bias field of the MR image before the intensity normalization step. Thus, the registration and the class distributions will be significantly enhanced. Since the registrationand class distributions is better, it should also improve the segmentation.
\par
To correct the bias field, we used the non-parametric approach presented by Sled \textit{et al} in \cite{19}. We choose a non-parametric approach because it does not require prior information like the number of tissues to correct or the mean value of each tissue to be corrected. We implement an ITK filter \cite{13} and \cite{14} in Slicer 3. 
\par
We can describe the new segmentation workflow in Slicer 3 as we do in Figure \ref{fig:wfwbc}.

\begin{figure}[ht]\centering
  \includegraphics[width=1\textwidth]{Images/Graphics/newalgo.png}
  \caption{New algorithm segmentation pipeline in Slicer 3}{First the intensity inhomogeneity are corrected. Second the intensity is normalized. Third an estimation of the transformation to apply for the registration is done. Fouth the atlases are aligned using the transformation. Finally, the EM segmentation algorithm is performed.}\label{fig:wfwbc}
  \end{figure}

\par
We choose not to implement it in Slicer 3 as part as the EM Segment module. Indeed, users may want to correct the bias field in MR images for other purposes. Moreover, it is possible because it would be the first pre-processing step. The user will first have to correct the intensity inhomogineity via the module developped for this particular purpose then use the corrected images in the EM segmentation module.

  
  
\par
After the bias correction, we obtain interesting results presented in Figure\ref{fig:goodRegistration}. (A) represents the atlas to be registered. (B) represents the target volume for the registration. (C) represents the atlas after the registration. The result of the registration visually appears to be better but we can not be satisfied of this visual assessment. We need a qualitative evaluation method.

\begin{figure}\centering
  \includegraphics[width=.5\textwidth]{Images/Screenshots/goodRegistration.png}
  \caption{Registration of a MR image without intensity inhomogeneity}{Image (A) is the atlas to be registered. Image (B) is the target of the registration. Image (C) is the image (A) after registration to image (B)}\label{fig:goodRegistration}
\end{figure}

\subsection{Evaluation}

We evaluated accuracy of the registration using the joint histograms method.
The joint histogram evaluation method is basic comparison between two images. Let $A$ be a matrix of size $WxL$. $W$ will be the intensity range of the first image used for the comparison. $L$ will be the intensity range of the second image to be compared. The matrix is initialized to zero. Each time that in the same position, there is the same intensities in the two images, we add one in the corresponding cell in the matrix. Thus, a perfect registration, would lead to an array of zeros, expect on the diagonal.
After the joint histogram creation, the value at the coordinate ${i,j}$ in the matrix is the number of pixel pairs having intensity $i$ in image onee and intensity $j$ in image two, at the same position in the volume.
\par
In Figure \ref{fig:joint1}, the left image (A) and the right image (B) compares the joint-histogram of the biased image and its atlas, respectively before and after registration.  The color scale used is the following one: if there are a lot of pixels in a cell of the array, the cell will be displayed in red. We the number of points decreases continously. The color scale goes from red to blue. The color fo medium intensities is yellow.

\begin{figure}\centering
  \includegraphics[width=.9\textwidth]{Images/Screenshots/jointhistograms1.png}
  \caption{Joint histograms to evaluate the registration of the inhomogeneous MR image}{Figure (A) represents the joint histogram of the inhomogeneous MR image and its atlas before registration. Figure (B) represents the joint histogram of the inhomogeneous MR image and its atlas after registration.}\label{fig:joint1}
\end{figure}


In Figure \ref{fig:joint2}, left image (A) and right image (B) compare the joint-histograms of the image after a bias field correction and its atlas, before and after registration.

\begin{figure}\centering
  \includegraphics[width=.9\textwidth]{Images/Screenshots/jointhistograms2.png}
  \caption{Joint histograms to evaluate the registration of the homogeneous MR image}{Figure (A) represents the joint histogram of the homogeneous MR image and its atlas before registration. Figure (B) represents the joint histogram of the homogeneous MR image and its atlas after registration.}\label{fig:joint2}
\end{figure}

It appears that the difference between before and after registration is more significant if the bias field is corrected. Indeed, we compare the joint histograms for volumes which exhibits intensity inhomogeneity: there is no significative difference between before and after registration. That means that the registration did not improve the similarity between the images. On the contrary, if we compare joint histogram for an intensity inhomogeneity free vollume, the number of points around the matrix diagonal increases. It means the images are more similiar after than before registration. It shows the utility of the contribution. The influence of these results after the whole segmentation process will be presented in the next chapter.
\par
Regarding the effect of the intensity inhomogeneity on tissue intensity distribution, we will present results using the tool we developped in the previous section, to evaluate the classe distribution.
Using the same labelmap for sampling, we obtain two significantly different distributions for a image corrupted with a bias field and the same image corrected. Figure \ref{fig:biasedunbiased} shows the two different distributions. The relation tissue/color is the same as the one in Figure \ref{fig:intensitynormalization}. The left image (A) presents the distribution before bias field correction. The right (B) presents the distribution in the corrected volume.

\begin{figure}\centering
  \includegraphics[width=.9\textwidth]{Images/Screenshots/biasedunbiased.png}
  \caption{Distribution of the class to be segmented in an inhomogeneous volume}{Left image (A) represents the class distribution in an homogeneous volume. Right image (B)represents the distribution of the class in a volume which exhibits intensity inhomogeneity. Air is plotted in dark blue, skull in blue, WM in yellow, GM in green and CSF in red. X axis represents a T1 volume and Y axis represents a T2 volume.}\label{fig:biasedunbiased}
\end{figure}
In the left image (A), the tissues variances are huge, especially for the CSF (red) in the T2 volume. According to an expert, it is not a good assumption. CSF should on appear in the high intensities of the T2 volume, as presented in image (B). Image (A) means that the CSF class contents pixels of high and low intensities in the T2 volume. It is a bad assumption and we can explain it because of the intensity inhomogeneity.
\par
Intuitively, we understand that the segmentation process should be a lot affected because of the two issues we presented (registration and distribution). Concrete results on the whole segmentation process will be presented in the next chapter.
%\subsection{Results}
%corrected not corrected
%parameters explanation
\subsection{Registration parameters}
Even if we are performing a non-parametric registration, some parameters have to be defined. "Non-parametric" means no information about the volume and classes to correct. We will first present and explain the parameters. In a second step, we will propose some parameters adjusted to different problems.

\begin{itemize}
\item \textbf{Shrink factor}\\
\hspace*{4 mm} It is a factor which is used to reduce the size of the image to be processed. A down-sampling is done by the bias correction filter.

\item \textbf{Maximum number of iterations}\\
\hspace*{4 mm} Optimization of the bias field occurs iteratively until the number of iterations exceeds the maximum specified by this variable.

\item \textbf{Bias field full width at maximum iteration}\\
\hspace*{4 mm} The bias field is modelled with a Gaussian. This variable characterizes this Gaussian (see \cite{20}) and can be presented as a parameter which defines the strength of the bias.
\end{itemize}

\par
From the understanding of the parameters, it makes sense that if the time of processing has tp be reduced, the shrink factor must be increased and the maximum number of iteration must be reduced. The limitation is that it can deteriore the bias field correction.
\par
The bias field full width at maximum iteration (BFFWMI) can also be reduced or increased, depending on the importance of the bias. If the bias field deteriorates significantly the image, an important BFFWMI must be used. On the contrary, if the intensity inhomogeneities are not significant, BFFWMI can be small.
%


\section{Intensity Normalization}\label{intensitynormalizationddd}
Another useful contribution is a tool which helps the user to determine the good normalization value.
\subsection{Motivation}
As discussed in section (\ref{GUI}), at the step 4, an intensity normalization is done. We have already presented the utility of an intensity normalization in the same section. The problem is that the user has no tool to find the optimum values for the segmentation. The user has to find the mean intensity of the voxels in the MR image, excluding the background. In practise it is challenging to estimate the good value since no tool exist for this particular purpose.
%
\subsection{Solution}
We implemented a tool, to allow the user to find easily and accurately this normalization value.
\par
The first step of the work consited in creating the histogram corresponding to the image. The $Y$ axis which presents the number of pixels for each intensity in the volume uses a log-scale because the range is large. The log scale significantly reduces the range. We then added a cursor in this histogram. With this feature, the user can choose the intensity which will be the threshold between the background and structure of interest. Finally, while the cursor is moving, our algorithm computes automatically the mean value of the voxels in the volume, from this intensity range to the highest intensity range. This is the normalization value.
\par
We present in Figure \ref{fig:intensitynormalization2} the tool we developped. A T1 volume has been loaded. The user can move the cursor in the histogram. While moving, it returns in real-time the normalization value for the given position of the cursor in the lower frame.

\begin{figure}\centering
  \includegraphics[width=0.5\textwidth]{Images/Screenshots/intensityNormalization.png}
  \caption{Tool developped for the intensity normalization parameter estimation}{The user moves the cursor and get an immediate feedback about the normalization value to use in the "recommended normal" box}\label{fig:intensitynormalization2}
\end{figure}
%


\section{Global Prior Weights Estimation}\label{GPSPDGSPGS}

The last contribution to the EM segmentation is a tool which provides the user an easy and fast way to estimate the approximate size (also called Global Prior Weights) of each class to be segmented.

\subsection{Motivation}
During the segmentation process,at the $6^{th}$ step of the intialization (section \ref{GUI}), the user has to provide to the algorithm an estimation of the size of each tissue to be segmented. 
First of all if there are a lot of structures to segment, this step is time consuming. Moreover, the user may not know at all which size to choose. A tool to estimate of the good sizes to choose is needed. 
%We must also keep in mind that the end users are physicists. They might don't understand what the parameters meanings and providing them a visual feedback could help them a lot.
\subsection{Solution}
Our solution is to divide the problem in two parts. We provide the user with a real-time feedback regarding the global prior weights estimation.
The second part consists of developping an algorithm which fills automatically the tree.
%\subsubsection{Fast user feedback}
%We can divide the feedback part in 3 steps: the histogram computation and utilisation, the multicolumn list and the labelmap generated.
%The histogram allows the user to manual segment classes based on intensity.
%The multicolumn list allows the user to change the order of the classes in the histogram.
%The labelmap provides to the user a visual feedback, base on the segmentation realized in the histogram.
%Using these three complementary tools, the user, even if he is not initiated can estimate easily, accurately and rapidely the GWP.
%\\schema
%\subsubsection{Global prior weights evaluation}
%The algorithm used to estimate the weight of each node is iterative. It starts from the root and goes to the leaves. It evaluates the weight of the childs of the active node at each iteration. Here is a description of the algorithms used to compute the GPW of each node.\\
%DEscirption algo 1\\\\
%
%
%\begin{minipage}{1\textwidth}
%
%\hrule
%\textbf{\\Algorithm 1:} \textsc{TreeWeightEstimation}(R, W)
%\hrule
%variables definition
%\textbf{\\define}  C = CHILD(R) $\leftarrow$ set of childrens of root R\\ 
%\textbf{define}  LEAF(C)      $\leftarrow$ set of leaves of tree with roots C\\ 
%\textbf{define}  H            $\leftarrow$ set of structure-specific information defined by LEAF(C) for each leaf\\
%
%\textbf{update} W in childrens of root R with the results of \textsc{WeightEstimation}(C,LEAF(C),H)\\
%
%\textbf{for each} node R' in CHILD(R) that is not a leaf\\
%
%$\triangleright$\textsc{TreeWeightEstimation}(R', W)\\
%\hrule
%
%\end{minipage}
%
%\\\\\\Descripton algo2\\
%The algorithm used estimates the global prior of the leaves of the current node, based on the number of pixel which belong to the child classes.
%This number of pixels is calculated from the segmentation computed in the histogram.(CF ..)\\\\
%%
%\begin{minipage}{1\textwidth}
%
%\hrule
%\textbf{\\Algorithm 2:} \textsc{WeightEstimation}(C,LEAF(C), W,H)%
%\hrule
%variables definition
%\textbf{\\define}  T $\leftarrow$ set of total weight of leaves in LEAF(C). Leaves weights are contained in H\\ 
%\textbf{define}  E      $\leftarrow$ set of weight for each node of C\\ 
%\textbf{for each}  node of C\\
%$\triangleright$E=E+H : Get the total weight of each node\\
%
%W=E/T\\
%
%\textbf{return} (W)\\
%\hrule

%\end{minipage}
%\\\\\\

  \begin{figure}\centering
  \includegraphics[width=0.7\textwidth]{Images/Screenshots/GlobalPrior.png}
  \caption{Tool developped for the global prior weights estimation}{Left image (A) presents the tool we developped for the global prior weights estimation. Right image (B) shows the visual feedback provided to the user given the tool presented in (A)}\label{fig:globalpriors}
  \end{figure}

The new tool is presented in Figure \ref{fig:globalpriors}, in the left image (A). When the user moves a cursor, it changes the intensity range for each class. The tool provides a real time feedback to the user. The user sees Figure \ref{fig:globalpriors}) (B) which is updated in real time, regarding the position of all the cursors consequence. Clicking on "update tree" (in Figure \ref{fig:globalpriors}) (A)), the estimated size of a class is computed and the information is stored into the tree structure (parameter $H$).\\ \\ \\ \\
\par
In this chapter we have presented our contribution to the EM segmentation algorithm. The illustrations shows that the selection of the parameter has been improved: in the next chapter we present the impact on the segmentation.

%As we can see, the results obtained are good and accurate.The time of processing is fast and most of the people are now able to understand these parameters, even they are not familiar with EMS. This is an intuitive way to get an estimation of the GPW parameters.\\
%Of course some sfsdfs must be done.
%If there is a strong bias in the images we process, the pixels of a same class will have different values. Then, a segmentation based on intensity will provide bad results. Moreover, if the user wants to segment 2 classes which have the same color, just using the spatial information for example, they won't be able to use it properly.Thus, the user must always keep in mind that it is just an estimation and that he has to check if the values are accurate.
%

